{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: c:\\Users\\Biebert\\OneDrive - Universität St.Gallen\\Dokumente\\OneDrive Dokumente\\02_Bildung\\01_BVWL Bachelor\\01_Kurse\\07_Thesis\\Code\\Portfolio_Optimization_DDPG\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Check if the original directory is already saved in a variable\n",
    "if 'original_directory' not in globals():\n",
    "    # Save the original working directory the first time\n",
    "    original_directory = os.getcwd()\n",
    "\n",
    "# Change back to the original directory whenever the cell is executed\n",
    "os.chdir(original_directory)\n",
    "\n",
    "# Go to mother directory\n",
    "os.chdir(\"../\")\n",
    "\n",
    "# Verify the current working directory\n",
    "print(\"Working directory set to:\", os.getcwd())\n",
    "\n",
    "sys.path.append(os.path.abspath(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_scripts import generate_data as gd\n",
    "from data_scripts import plotting as pl\n",
    "import datetime\n",
    "\n",
    "tickers = ['MSFT', 'TGT', 'QCOM', 'MU', 'CAT']\n",
    "\n",
    "# Load simulation data for the selected stocks\n",
    "stock_data_dict = gd.load_simulation_data(tickers)\n",
    "\n",
    "# Get the combined training simulation data for the selected stocks\n",
    "combined_train_data = gd.get_combined_simulation(stock_data_dict, simulation_index=0, set_type='train')\n",
    "\n",
    "\n",
    "# Assuming combined_train_data is a NumPy array or a DataFrame\n",
    "num_days_train, num_ep_train = data = stock_data_dict[tickers[0]]['train'].shape\n",
    "num_days_test, num_ep_test = data = stock_data_dict[tickers[0]]['test'].shape\n",
    "\n",
    "def generate_time():\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return f\"{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  33%|███▎      | 1000/3001 [5:48:02<11:53:45, 21.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  67%|██████▋   | 2000/3001 [12:04:37<5:28:09, 19.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|█████████▉| 3000/3001 [16:51:34<00:16, 16.51s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 3001/3001 [16:53:17<00:00, 20.26s/it]\n"
     ]
    }
   ],
   "source": [
    "#raise Exception(\"Skip this cell\")\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from trading_envs.trading_env import TradingEnv\n",
    "from models.ddpg_agent import Agent\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_num = 8100\n",
    "\n",
    "# Initialize variables\n",
    "train_simulations = num_ep_train\n",
    "test_simulations = num_ep_test\n",
    "train_days = num_days_train\n",
    "test_days = num_days_test\n",
    "\n",
    "num_episodes = 3001\n",
    "max_steps_per_episode = train_days \n",
    "\n",
    "reward_function = 'portfolio_value'\n",
    "\n",
    "# Initialize the environment and the agent\n",
    "train_simulation_data = gd.get_combined_simulation(stock_data_dict, simulation_index=0, set_type='train')\n",
    "env = TradingEnv(stock_data=train_simulation_data, reward_function=reward_function)\n",
    "agent = Agent(alpha=0.0001, beta=0.001, input_dims=[env.observation_space.shape[0]], \n",
    "                tau=0.001, env=env, batch_size=64, layer1_size=400, layer2_size=300, \n",
    "                n_actions=env.action_space.shape[0])\n",
    "\n",
    "agent.load_models(suffix='8804_20241029_015052_portfolio_value')\n",
    "\n",
    "# Initialize DataFrames\n",
    "episode_scores_df = pd.DataFrame(columns=['Episode', 'Score'])\n",
    "wealth_df = pd.DataFrame(columns=['Episode', 'TimeStep', 'Simulation', 'Wealth'])\n",
    "actions_columns = ['Episode', 'TimeStep'] + [f'Action_{i}' for i in range(env.action_space.shape[0])]\n",
    "actions_df = pd.DataFrame(columns=actions_columns)\n",
    "states_columns = ['Episode', 'TimeStep'] + [f'State_{i}' for i in range(env.observation_space.shape[0])]\n",
    "states_df = pd.DataFrame(columns=states_columns)\n",
    "rewards_df = pd.DataFrame(columns=['Episode', 'TimeStep', 'Reward'])\n",
    "\n",
    "# Training loop only one time\n",
    "for episode in tqdm(range(num_episodes), desc=\"Training Progress\"):\n",
    "    episode_wealth = []\n",
    "    episode_actions = []\n",
    "    episode_states = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Reset environment and variables\n",
    "    train_simulation_index = random.choice(range(train_simulations))\n",
    "    train_simulation_data = gd.get_combined_simulation(stock_data_dict, simulation_index=train_simulation_index, set_type='train')\n",
    "    # Set the new environment with this training data\n",
    "    env = TradingEnv(stock_data=train_simulation_data, reward_function=reward_function)\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    time_step = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, _, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, state_, done)\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "\n",
    "        # Collect data\n",
    "        episode_wealth.append({\n",
    "            'Episode': episode, \n",
    "            'TimeStep': time_step, \n",
    "            'Simulation': train_simulation_index,\n",
    "            'Wealth': env.get_portfolio_value()\n",
    "        })\n",
    "\n",
    "        action_record = {'Episode': episode, 'TimeStep': time_step}\n",
    "        action_record.update({f'Action_{i}': a for i, a in enumerate(action)})\n",
    "        episode_actions.append(action_record)\n",
    "\n",
    "        state_record = {'Episode': episode, 'TimeStep': time_step}\n",
    "        state_record.update({f'State_{i}': s for i, s in enumerate(state)})\n",
    "        episode_states.append(state_record)\n",
    "\n",
    "        episode_rewards.append({\n",
    "            'Episode': episode, \n",
    "            'TimeStep': time_step, \n",
    "            'Reward': reward\n",
    "        })\n",
    "\n",
    "        state = state_\n",
    "        time_step += 1\n",
    "\n",
    "    # Append episode data to DataFrames\n",
    "    episode_scores_df = pd.concat([episode_scores_df, pd.DataFrame([{'Episode': episode, 'Score': score}])], ignore_index=True)\n",
    "    wealth_df = pd.concat([wealth_df, pd.DataFrame(episode_wealth)], ignore_index=True)\n",
    "    actions_df = pd.concat([actions_df, pd.DataFrame(episode_actions)], ignore_index=True)\n",
    "    states_df = pd.concat([states_df, pd.DataFrame(episode_states)], ignore_index=True)\n",
    "    rewards_df = pd.concat([rewards_df, pd.DataFrame(episode_rewards)], ignore_index=True)\n",
    "\n",
    "    if episode % 1000 == 0 and episode > 999:\n",
    "\n",
    "\n",
    "        time = generate_time()\n",
    "        time_first = time\n",
    "        agent.save_models(suffix=f'{save_num}_{time}_{reward_function}')\n",
    "\n",
    "        # Save with unique filename\n",
    "        episode_scores_df.to_csv(f'data_save/{save_num}_episode_scores_{time}_{reward_function}.csv', index=False)\n",
    "        wealth_df.to_csv(f'data_save/{save_num}_wealth_over_time_{time}_{reward_function}.csv', index=False)\n",
    "        actions_df.to_csv(f'data_save/{save_num}_actions_taken_{time}_{reward_function}.csv', index=False)\n",
    "        states_df.to_csv(f'data_save/{save_num}_states_observed_{time}_{reward_function}.csv', index=False)\n",
    "        rewards_df.to_csv(f'data_save/{save_num}_rewards_received_{time}_{reward_function}.csv', index=False)\n",
    "\n",
    "        save_num += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  33%|███▎      | 1000/3001 [4:54:51<10:59:20, 19.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  67%|██████▋   | 2000/3001 [10:13:46<4:57:37, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|█████████▉| 3000/3001 [16:07:51<00:28, 28.48s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 3001/3001 [16:11:17<00:00, 19.42s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from trading_envs.trading_env import TradingEnv\n",
    "from models.ddpg_agent import Agent\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_num = 9100\n",
    "\n",
    "# Initialize variables\n",
    "train_simulations = num_ep_train\n",
    "test_simulations = num_ep_test\n",
    "train_days = num_days_train\n",
    "test_days = num_days_test\n",
    "\n",
    "num_episodes = 3001\n",
    "max_steps_per_episode = train_days \n",
    "\n",
    "reward_function = 'diff_sharpe_reward'\n",
    "\n",
    "\n",
    "# Initialize the environment and the agent\n",
    "train_simulation_data = gd.get_combined_simulation(stock_data_dict, simulation_index=1, set_type='train')\n",
    "env = TradingEnv(stock_data=train_simulation_data, reward_function=reward_function)\n",
    "agent = Agent(alpha=0.0001, beta=0.001, input_dims=[env.observation_space.shape[0]], \n",
    "                tau=0.001, env=env, batch_size=64, layer1_size=400, layer2_size=300, \n",
    "                n_actions=env.action_space.shape[0])\n",
    "\n",
    "agent.load_models(suffix='9904_20241029_232625_diff_sharpe_reward')\n",
    "\n",
    "# Initialize DataFrames\n",
    "episode_scores_df = pd.DataFrame(columns=['Episode', 'Score'])\n",
    "wealth_df = pd.DataFrame(columns=['Episode', 'TimeStep', 'Simulation', 'Wealth'])\n",
    "actions_columns = ['Episode', 'TimeStep'] + [f'Action_{i}' for i in range(env.action_space.shape[0])]\n",
    "actions_df = pd.DataFrame(columns=actions_columns)\n",
    "states_columns = ['Episode', 'TimeStep'] + [f'State_{i}' for i in range(env.observation_space.shape[0])]\n",
    "states_df = pd.DataFrame(columns=states_columns)\n",
    "rewards_df = pd.DataFrame(columns=['Episode', 'TimeStep', 'Reward'])\n",
    "\n",
    "# Training loop only one time\n",
    "for episode in tqdm(range(num_episodes), desc=\"Training Progress\"):\n",
    "    episode_wealth = []\n",
    "    episode_actions = []\n",
    "    episode_states = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Reset environment and variables\n",
    "    train_simulation_index = random.choice(range(train_simulations))\n",
    "    train_simulation_data = gd.get_combined_simulation(stock_data_dict, simulation_index=train_simulation_index, set_type='train')\n",
    "    \n",
    "    # Set the new environment with this training data\n",
    "    env = TradingEnv(stock_data=train_simulation_data, reward_function=reward_function)\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    time_step = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, _, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, state_, done)\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "\n",
    "        # Collect data\n",
    "        episode_wealth.append({\n",
    "            'Episode': episode, \n",
    "            'TimeStep': time_step, \n",
    "            'Simulation': train_simulation_index,\n",
    "            'Wealth': env.get_portfolio_value()\n",
    "        })\n",
    "\n",
    "        action_record = {'Episode': episode, 'TimeStep': time_step}\n",
    "        action_record.update({f'Action_{i}': a for i, a in enumerate(action)})\n",
    "        episode_actions.append(action_record)\n",
    "\n",
    "        state_record = {'Episode': episode, 'TimeStep': time_step}\n",
    "        state_record.update({f'State_{i}': s for i, s in enumerate(state)})\n",
    "        episode_states.append(state_record)\n",
    "\n",
    "        episode_rewards.append({\n",
    "            'Episode': episode, \n",
    "            'TimeStep': time_step, \n",
    "            'Reward': reward\n",
    "        })\n",
    "\n",
    "        state = state_\n",
    "        time_step += 1\n",
    "\n",
    "    # Append episode data to DataFrames\n",
    "    episode_scores_df = pd.concat([episode_scores_df, pd.DataFrame([{'Episode': episode, 'Score': score}])], ignore_index=True)\n",
    "    wealth_df = pd.concat([wealth_df, pd.DataFrame(episode_wealth)], ignore_index=True)\n",
    "    actions_df = pd.concat([actions_df, pd.DataFrame(episode_actions)], ignore_index=True)\n",
    "    states_df = pd.concat([states_df, pd.DataFrame(episode_states)], ignore_index=True)\n",
    "    rewards_df = pd.concat([rewards_df, pd.DataFrame(episode_rewards)], ignore_index=True)\n",
    "\n",
    "    if episode % 1000 == 0 and episode > 999:\n",
    "\n",
    "\n",
    "        time = generate_time()\n",
    "        time_first = time\n",
    "        agent.save_models(suffix=f'{save_num}_{time}_{reward_function}')\n",
    "\n",
    "        # Save with unique filename\n",
    "        episode_scores_df.to_csv(f'data_save/{save_num}_episode_scores_{time}_{reward_function}.csv', index=False)\n",
    "        wealth_df.to_csv(f'data_save/{save_num}_wealth_over_time_{time}_{reward_function}.csv', index=False)\n",
    "        actions_df.to_csv(f'data_save/{save_num}_actions_taken_{time}_{reward_function}.csv', index=False)\n",
    "        states_df.to_csv(f'data_save/{save_num}_states_observed_{time}_{reward_function}.csv', index=False)\n",
    "        rewards_df.to_csv(f'data_save/{save_num}_rewards_received_{time}_{reward_function}.csv', index=False)\n",
    "\n",
    "        save_num += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the surplus and the market and agent wealth for each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulations Progress: 100%|██████████| 5000/5000 [03:47<00:00, 21.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from data_scripts import generate_data as gd\n",
    "from data_scripts import plotting as pl\n",
    "\n",
    "# Initialize an empty list to store the surplus values for each simulation\n",
    "surplus = []\n",
    "markets = []\n",
    "agents = []\n",
    "\n",
    "# Read csv data once \n",
    "#agent_wealth = pd.read_csv(f'data_save/9904_wealth_over_time_20241029_232625_diff_sharpe_reward.csv')\n",
    "#states = pd.read_csv(f'data_save/9904_states_observed_20241029_232625_diff_sharpe_reward_value.csv')\n",
    "\n",
    "agent_wealth = pd.read_csv(f'data_save/8804_wealth_over_time_20241029_015052_portfolio_value.csv')\n",
    "states = pd.read_csv(f'data_save/8804_states_observed_20241029_015052_portfolio_value.csv')\n",
    "\n",
    "\n",
    "#agent_wealth = pd.read_csv(f'data_save/100004_wealth_over_time_20241025_183434_portfolio_value.csv')\n",
    "#states = pd.read_csv(f'data_save/100004_states_observed_20241025_183434_portfolio_value.csv')\n",
    "\n",
    "# Loop over each simulation index\n",
    "for i in tqdm(range(agent_wealth['Episode'].max()), desc=\"Simulations Progress\"):\n",
    "    # Filter agent wealth data for the current episode once\n",
    "    agent_wealth_e = agent_wealth[agent_wealth['Episode'] == i].set_index('TimeStep')\n",
    "    prices = states[states['Episode'] == i].set_index('TimeStep').filter(like='State_').values[:, :6]\n",
    "\n",
    "    # Preallocate the portfolio surplus list\n",
    "    portfolio_surplus = np.empty(len(prices))\n",
    "    market = np.empty(len(prices))\n",
    "    agent = np.empty(len(prices))\n",
    "\n",
    "    # Iterate over the time steps\n",
    "    for n in range(0, len(prices)):\n",
    "        # Retrieve agent wealth data for the current timestep\n",
    "        agent_wealth_day = agent_wealth_e.loc[n, 'Wealth']\n",
    "\n",
    "        # Calculate the current market value\n",
    "        market_price_day = prices[n]\n",
    "\n",
    "        # Distribute 1000 equally among the 5 stocks and cash with the prices from teh first day\n",
    "        dist_vec = 1000/ len(market_price_day) / prices[0]\n",
    "\n",
    "        market_wealth_day = np.sum(market_price_day * dist_vec)\n",
    "\n",
    "        # Calculate wealth difference (agent wealth - calculated wealth)\n",
    "        wealth_diff = agent_wealth_day - market_wealth_day\n",
    "\n",
    "        # Store the wealth difference\n",
    "        portfolio_surplus[n] = np.round(wealth_diff, 2)\n",
    "        market[n] = np.round(market_wealth_day, 2)  # Round market value to 2 decimal places\n",
    "        agent[n] = np.round(agent_wealth_day, 2)\n",
    "\n",
    "\n",
    "    # Append the portfolio surplus for this simulation to the overall surplus list\n",
    "    surplus.append(portfolio_surplus)\n",
    "    markets.append(market)\n",
    "    agents.append(agent)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surplus in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulations Progress: 100%|██████████| 100/100 [00:00<00:00, 1901.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "def plot_surplus(surplus, title, start_index=0):\n",
    "    # Create a figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Set up colors ranging from light to dark blue\n",
    "    num_episodes = len(surplus)\n",
    "    color_gradient = np.linspace(0.2, 1, num_episodes)  # Generates values from 0.2 (light blue) to 1 (dark blue)\n",
    "\n",
    "    # Plot each episode's surplus data with progressively darker blue\n",
    "    for i in tqdm(range(start_index, num_episodes), desc=\"Simulations Progress\"):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            y=surplus[i],\n",
    "            mode='lines',\n",
    "            name=f'Episode {i+1}',\n",
    "            line=dict(color=f'rgba(0, 0, 255, {color_gradient[i]})', width=2)  # Increasingly dark blue\n",
    "        ))\n",
    "\n",
    "    # Update layout to make it interactive and clear\n",
    "    fig.update_layout(\n",
    "        title=\"Surplus Over Time for Each Episode First\",\n",
    "        xaxis_title=\"Time Step\",\n",
    "        yaxis_title=\"Surplus\",\n",
    "        hovermode=\"x unified\",  # Shows all episode values at a given timestep\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    # Show the interactive plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "start_idx = max(0, 4900)\n",
    "plot_surplus(surplus, \"Surplus Over Time for Each Episode First\", start_index=start_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "# Function to plot the average agent wealth and market wealth across all simulations\n",
    "def plot_average_wealth(surplus, markets, agents, title, start_index=0):\n",
    "    # Calculate the average market and agent wealth across all simulations\n",
    "    recent_markets = markets[start_index:]\n",
    "    recent_agents = agents[start_index:]\n",
    "\n",
    "    avg_market_wealth = np.mean(recent_markets, axis=0)\n",
    "    avg_agent_wealth = np.mean(recent_agents, axis=0)\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot average market wealth\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=avg_market_wealth,\n",
    "            mode='lines',\n",
    "            name='Average Market Wealth',\n",
    "            line=dict(color='blue', width=2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot average agent wealth\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=avg_agent_wealth,\n",
    "            mode='lines',\n",
    "            name='Average Agent Wealth',\n",
    "            line=dict(color='green', width=2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title + str(len(recent_markets))+ ' Simulations',\n",
    "        xaxis_title=\"Time Steps\",\n",
    "        yaxis_title=\"Wealth\",\n",
    "        hovermode=\"x unified\",  # Improves interactivity\n",
    "        showlegend=True,\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    # Show the interactive plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "start_idx = max(0, 4900)\n",
    "plot_average_wealth(surplus,markets,agents, \"Average Market and Agent Wealth Across \", start_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Sharpe Ratios: 100%|██████████| 5000/5000 [00:00<00:00, 11204.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sharpe Ratio for Agent across all simulations: 0.0525\n",
      "Average Sharpe Ratio for Market across all simulations: 0.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists to store Sharpe ratios for each episode for both the agent and the market\n",
    "agent_sharpe_ratios = []\n",
    "market_sharpe_ratios = []\n",
    "\n",
    "# Loop over each episode using the already populated `markets` and `agents` lists\n",
    "for episode_index in tqdm(range(len(markets)), desc=\"Calculating Sharpe Ratios\"):\n",
    "    # Get the agent and market values for the current episode\n",
    "    agent_values = agents[episode_index]\n",
    "    market_values = markets[episode_index]\n",
    "    \n",
    "    # Calculate returns for agent portfolio and market for each timestep in this episode\n",
    "    agent_returns = np.diff(agent_values) / agent_values[:-1]\n",
    "    market_returns = np.diff(market_values) / market_values[:-1]\n",
    "    \n",
    "    # Calculate the Sharpe ratio for the agent's portfolio in this episode\n",
    "    agent_sharpe_ratio = agent_returns.mean() / agent_returns.std() if agent_returns.std() != 0 else 0\n",
    "    agent_sharpe_ratios.append(agent_sharpe_ratio)\n",
    "    \n",
    "    # Calculate the Sharpe ratio for the market in this episode\n",
    "    market_sharpe_ratio = market_returns.mean() / market_returns.std() if market_returns.std() != 0 else 0\n",
    "    market_sharpe_ratios.append(market_sharpe_ratio)\n",
    "\n",
    "# Calculate the average Sharpe ratios across all episodes for both agent and market\n",
    "average_agent_sharpe_ratio = np.mean(agent_sharpe_ratios)\n",
    "average_market_sharpe_ratio = np.mean(market_sharpe_ratios)\n",
    "\n",
    "# Print the comparison\n",
    "print(f\"Average Sharpe Ratio for Agent across all simulations: {average_agent_sharpe_ratio:.4f}\")\n",
    "print(f\"Average Sharpe Ratio for Market across all simulations: {average_market_sharpe_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surplus in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_scripts import generate_data as gd\n",
    "from data_scripts import plotting as pl\n",
    "import datetime\n",
    "\n",
    "tickers = ['MSFT', 'TGT', 'QCOM', 'MU', 'CAT']\n",
    "\n",
    "# Load simulation data for the selected stocks\n",
    "stock_data_dict = gd.load_simulation_data(tickers)\n",
    "\n",
    "# Get the combined training simulation data for the selected stocks\n",
    "combined_train_data = gd.get_combined_simulation(stock_data_dict, simulation_index=0, set_type='train')\n",
    "\n",
    "\n",
    "# Assuming combined_train_data is a NumPy array or a DataFrame\n",
    "num_days_train, num_ep_train = data = stock_data_dict[tickers[0]]['train'].shape\n",
    "num_days_test, num_ep_test = data = stock_data_dict[tickers[0]]['test'].shape\n",
    "\n",
    "def generate_time():\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return f\"{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Progress: 100%|██████████| 100/100 [25:10<00:00, 15.10s/it]\n"
     ]
    }
   ],
   "source": [
    "#raise Exception(\"Skip this cell\")\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from trading_envs.trading_env import TradingEnv\n",
    "from models.ddpg_agent import Agent\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Test on new test data\n",
    "reward_function = 'portfolio_value'\n",
    "test_simulations = num_ep_test\n",
    "test_days = num_days_test\n",
    "test_simulation_data = gd.get_combined_simulation(stock_data_dict, simulation_index=0, set_type='test')\n",
    "env_test = TradingEnv(stock_data=test_simulation_data, reward_function=reward_function)\n",
    "\n",
    "\n",
    "agent = Agent(alpha=0.0001, beta=0.001, input_dims=[env_test.observation_space.shape[0]], \n",
    "                tau=0.001, env=env_test, batch_size=64, layer1_size=400, layer2_size=300, \n",
    "                n_actions=env_test.action_space.shape[0])\n",
    "\n",
    "# Load the trained model # actor_ddpg100010_20241026_163341_portfolio_value \n",
    "agent.load_models(suffix=f'8804_20241029_015052_portfolio_value')  # Load the most recent saved model\n",
    "\n",
    "actions_columns = ['Episode', 'TimeStep'] + [f'Action_{i}' for i in range(env_test.action_space.shape[0])]\n",
    "\n",
    "# DataFrames to store test results\n",
    "test_wealth_df = pd.DataFrame(columns=['Episode', 'TimeStep', 'Simulation', 'Wealth'])\n",
    "test_actions_df = pd.DataFrame(columns=actions_columns)\n",
    "test_rewards_df = pd.DataFrame(columns=['Episode', 'TimeStep', 'Reward'])\n",
    "states_columns = ['Episode', 'TimeStep'] + [f'State_{i}' for i in range(env_test.observation_space.shape[0])]\n",
    "test_states_df = pd.DataFrame(columns=states_columns)\n",
    "\n",
    "# Training loop only one time\n",
    "for episode in tqdm(range(test_simulations), desc=\"Testing Progress\"):\n",
    "    episode_wealth = []\n",
    "    episode_actions = []\n",
    "    episode_states = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Reset environment and variables\n",
    "    test_simulation_data = gd.get_combined_simulation(stock_data_dict, simulation_index=episode, set_type='test')\n",
    "    # Set the new environment with this training data\n",
    "    env = TradingEnv(stock_data=test_simulation_data, reward_function=reward_function)\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    time_step = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, _, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, state_, done)\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "\n",
    "        # Collect data\n",
    "        episode_wealth.append({\n",
    "            'Episode': episode, \n",
    "            'TimeStep': time_step, \n",
    "            'Simulation': episode,\n",
    "            'Wealth': env.get_portfolio_value()\n",
    "        })\n",
    "\n",
    "        state_record = {'Episode': episode, 'TimeStep': time_step}\n",
    "        state_record.update({f'State_{i}': s for i, s in enumerate(state)})\n",
    "        episode_states.append(state_record)\n",
    "\n",
    "        state = state_\n",
    "        time_step += 1\n",
    "\n",
    "    # Append episode data to DataFrames\n",
    "    test_wealth_df = pd.concat([test_wealth_df, pd.DataFrame(episode_wealth)], ignore_index=True)\n",
    "    test_states_df = pd.concat([test_states_df, pd.DataFrame(episode_states)], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "time = generate_time()\n",
    "time_first = time\n",
    "\n",
    "# Save with unique filename\n",
    "test_wealth_df.to_csv(f'data_save/wealth_over_time_test_{time}_{reward_function}.csv', index=False)\n",
    "test_states_df.to_csv(f'data_save/states_observed_test_{time}_{reward_function}.csv', index=False)\n",
    "\n",
    "save_num += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulations Progress: 100%|██████████| 99/99 [00:08<00:00, 11.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store surplus, market, and agent values for each simulation\n",
    "surplus = []\n",
    "markets = []\n",
    "agents = []\n",
    "\n",
    "# Load CSV data once\n",
    "agent_wealth = pd.read_csv('data_save/wealth_over_time_test_20241030_011304_portfolio_value.csv')\n",
    "states = pd.read_csv('data_save/states_observed_test_20241030_011304_portfolio_value.csv')\n",
    "\n",
    "# wealth_over_time_test_20241030_101410_diff_sharpe_reward\n",
    "\n",
    "#agent_wealth = pd.read_csv(f'data_save/wealth_over_time_test_{time}_{reward_function}.csv')\n",
    "#states = pd.read_csv(f'data_save/states_observed_test_{time}_{reward_function}.csv')\n",
    "\n",
    "# Loop over each simulation index\n",
    "for i in tqdm(range(agent_wealth['Episode'].max()), desc=\"Simulations Progress\"):\n",
    "    # Filter agent wealth data for the current episode\n",
    "    agent_wealth_e = agent_wealth[agent_wealth['Episode'] == i].set_index('TimeStep')\n",
    "    prices = states[states['Episode'] == i].set_index('TimeStep').filter(like='State_').values[:, :6]\n",
    "\n",
    "    # Preallocate arrays for each simulation's surplus\n",
    "    portfolio_surplus = np.empty(len(prices))\n",
    "    market = np.empty(len(prices))\n",
    "    agent = np.empty(len(prices))\n",
    "\n",
    "    # Iterate over the time steps\n",
    "    for n in range(len(prices)):\n",
    "        # Retrieve agent wealth data for the current timestep\n",
    "        agent_wealth_day = agent_wealth_e.loc[n, 'Wealth']\n",
    "        \n",
    "        # Ensure it's a scalar value\n",
    "        if isinstance(agent_wealth_day, pd.Series):\n",
    "            agent_wealth_day = agent_wealth_day.iloc[0]\n",
    "\n",
    "        # Calculate the current market value\n",
    "        market_price_day = prices[n]\n",
    "\n",
    "        # Distribute 1000 equally among the 5 stocks and cash with prices from the first day\n",
    "        dist_vec = 1000 / len(market_price_day) / prices[0]\n",
    "        market_wealth_day = np.sum(market_price_day * dist_vec)\n",
    "\n",
    "        # Ensure market_wealth_day is a scalar value\n",
    "        if isinstance(market_wealth_day, np.ndarray):\n",
    "            market_wealth_day = market_wealth_day.item()\n",
    "\n",
    "        # Calculate wealth difference (agent wealth - calculated wealth)\n",
    "        wealth_diff = agent_wealth_day - market_wealth_day\n",
    "\n",
    "        # Store the wealth difference in the preallocated arrays\n",
    "        portfolio_surplus[n] = np.round(wealth_diff, 2)\n",
    "        market[n] = np.round(market_wealth_day, 2)\n",
    "        agent[n] = np.round(agent_wealth_day, 2)\n",
    "\n",
    "    # Append the results for this simulation to the overall lists\n",
    "    surplus.append(portfolio_surplus)\n",
    "    markets.append(market)\n",
    "    agents.append(agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "# Function to plot the average agent wealth and market wealth across all simulations\n",
    "def plot_average_wealth(surplus, markets, agents, title, start_index=0):\n",
    "    # Calculate the average market and agent wealth across all simulations\n",
    "    recent_markets = markets[start_index:]\n",
    "    recent_agents = agents[start_index:]\n",
    "\n",
    "    avg_market_wealth = np.mean(recent_markets, axis=0)\n",
    "    avg_agent_wealth = np.mean(recent_agents, axis=0)\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot average market wealth\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=avg_market_wealth,\n",
    "            mode='lines',\n",
    "            name='Average Market Wealth',\n",
    "            line=dict(color='blue', width=2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot average agent wealth\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=avg_agent_wealth,\n",
    "            mode='lines',\n",
    "            name='Average Agent Wealth',\n",
    "            line=dict(color='green', width=2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title + str(len(recent_markets))+ ' Simulations',\n",
    "        xaxis_title=\"Time Steps\",\n",
    "        yaxis_title=\"Wealth\",\n",
    "        hovermode=\"x unified\",  # Improves interactivity\n",
    "        showlegend=True,\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    # Show the interactive plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "start_idx = max(0, 0)\n",
    "plot_average_wealth(surplus,markets,agents, \"Average Market and Agent Wealth Across test\", start_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Sharpe Ratios: 100%|██████████| 99/99 [00:00<00:00, 4524.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 Average Sharpe Ratio for Agent across all simulations: 0.0543\n",
      "Testing 1 Average Sharpe Ratio for Market across all simulations: 0.0778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists to store Sharpe ratios for each episode for both the agent and the market\n",
    "agent_sharpe_ratios = []\n",
    "market_sharpe_ratios = []\n",
    "\n",
    "# Loop over each episode using the already populated `markets` and `agents` lists\n",
    "for episode_index in tqdm(range(len(markets)), desc=\"Calculating Sharpe Ratios\"):\n",
    "    # Get the agent and market values for the current episode\n",
    "    agent_values = agents[episode_index]\n",
    "    market_values = markets[episode_index]\n",
    "    \n",
    "    # Calculate returns for agent portfolio and market for each timestep in this episode\n",
    "    agent_returns = np.diff(agent_values) / agent_values[:-1]\n",
    "    market_returns = np.diff(market_values) / market_values[:-1]\n",
    "    \n",
    "    # Calculate the Sharpe ratio for the agent's portfolio in this episode\n",
    "    agent_sharpe_ratio = agent_returns.mean() / agent_returns.std() if agent_returns.std() != 0 else 0\n",
    "    agent_sharpe_ratios.append(agent_sharpe_ratio)\n",
    "    \n",
    "    # Calculate the Sharpe ratio for the market in this episode\n",
    "    market_sharpe_ratio = market_returns.mean() / market_returns.std() if market_returns.std() != 0 else 0\n",
    "    market_sharpe_ratios.append(market_sharpe_ratio)\n",
    "\n",
    "# Calculate the average Sharpe ratios across all episodes for both agent and market\n",
    "average_agent_sharpe_ratio = np.mean(agent_sharpe_ratios)\n",
    "average_market_sharpe_ratio = np.mean(market_sharpe_ratios)\n",
    "\n",
    "# Print the comparison\n",
    "print(f\"Testing 1 Average Sharpe Ratio for Agent across all simulations: {average_agent_sharpe_ratio:.4f}\")\n",
    "print(f\"Testing 1 Average Sharpe Ratio for Market across all simulations: {average_market_sharpe_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulations Progress: 100%|██████████| 3000/3000 [03:05<00:00, 16.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from data_scripts import generate_data as gd\n",
    "from data_scripts import plotting as pl\n",
    "\n",
    "# Initialize an empty list to store the surplus values for each simulation\n",
    "surplus_2 = []\n",
    "markets_2 = []\n",
    "agents_2 = []\n",
    "\n",
    "# Read csv data once ###200011_20241027_145013_diff_sharpe_reward\n",
    "agent_wealth_2 = pd.read_csv(f'data_save/9102_wealth_over_time_20241101_210054_diff_sharpe_reward.csv')\n",
    "states_2 = pd.read_csv(f'data_save/9102_states_observed_20241101_210054_diff_sharpe_reward.csv')\n",
    "\n",
    "# Loop over each simulation index\n",
    "for i in tqdm(range(agent_wealth_2['Episode'].max()), desc=\"Simulations Progress\"):\n",
    "    # Filter agent wealth data for the current episode once\n",
    "    agent_wealth_e_2 = agent_wealth_2[agent_wealth_2['Episode'] == i].set_index('TimeStep')\n",
    "    prices_2 = states_2[states_2['Episode'] == i].set_index('TimeStep').filter(like='State_').values[:, :6]\n",
    "\n",
    "    # Preallocate the portfolio surplus list\n",
    "    portfolio_surplus_2 = np.empty(len(prices_2))\n",
    "    market_2 = np.empty(len(prices_2))\n",
    "    agent_2 = np.empty(len(prices_2))\n",
    "\n",
    "    # Iterate over the time steps\n",
    "    for n in range(0, len(prices)):\n",
    "        # Retrieve agent wealth data for the current timestep\n",
    "        agent_wealth_day_2 = agent_wealth_e_2.loc[n, 'Wealth']\n",
    "\n",
    "        # Calculate the current market value\n",
    "        market_price_day_2 = prices_2[n]\n",
    "\n",
    "        # Distribute 1000 equally among the 5 stocks and cash with the prices from teh first day\n",
    "        dist_vec_2 = 1000/ len(market_price_day_2) / prices_2[0]\n",
    "\n",
    "        market_wealth_day_2 = np.sum(market_price_day_2 * dist_vec_2)\n",
    "\n",
    "        # Calculate wealth difference (agent wealth - calculated wealth)\n",
    "        wealth_diff_2 = agent_wealth_day_2 - market_wealth_day_2\n",
    "\n",
    "        # Store the wealth difference\n",
    "        portfolio_surplus_2[n] = np.round(wealth_diff_2, 2)\n",
    "        market_2[n] = np.round(market_wealth_day_2, 2)  # Round market value to 2 decimal places\n",
    "        agent_2[n] = np.round(agent_wealth_day_2, 2)\n",
    "\n",
    "\n",
    "    # Append the portfolio surplus for this simulation to the overall surplus list\n",
    "    surplus_2.append(portfolio_surplus_2)\n",
    "    markets_2.append(market_2)\n",
    "    agents_2.append(agent_2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surplus training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulations Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulations Progress: 100%|██████████| 100/100 [00:00<00:00, 696.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "def plot_surplus(surplus, title, start_index=0):\n",
    "    # Create a figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Set up colors ranging from light to dark blue\n",
    "    num_episodes = len(surplus)\n",
    "    color_gradient = np.linspace(0.2, 1, num_episodes)  # Generates values from 0.2 (light blue) to 1 (dark blue)\n",
    "\n",
    "    # Plot each episode's surplus data with progressively darker blue\n",
    "    for i in tqdm(range(start_index, num_episodes), desc=\"Simulations Progress\"):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            y=surplus[i],\n",
    "            mode='lines',\n",
    "            name=f'Episode {i+1}',\n",
    "            line=dict(color=f'rgba(0, 0, 255, {color_gradient[i]})', width=2)  # Increasingly dark blue\n",
    "        ))\n",
    "\n",
    "    # Update layout to make it interactive and clear\n",
    "    fig.update_layout(\n",
    "        title=\"Surplus Over Time for Each Episode First\",\n",
    "        xaxis_title=\"Time Step\",\n",
    "        yaxis_title=\"Surplus\",\n",
    "        hovermode=\"x unified\",  # Shows all episode values at a given timestep\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    # Show the interactive plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "start_idx = max(0, 2900)\n",
    "plot_surplus(surplus_2, \"Surplus Over Time for Each Episode Second\", start_index=start_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "# Function to plot the average agent wealth and market wealth across all simulations\n",
    "def plot_average_wealth(surplus, markets, agents, title, start_index=0):\n",
    "    # Calculate the average market and agent wealth across all simulations\n",
    "    recent_markets = markets[start_index:]\n",
    "    recent_agents = agents[start_index:]\n",
    "\n",
    "    avg_market_wealth = np.mean(recent_markets, axis=0)\n",
    "    avg_agent_wealth = np.mean(recent_agents, axis=0)\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot average market wealth\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=avg_market_wealth,\n",
    "            mode='lines',\n",
    "            name='Average Market Wealth',\n",
    "            line=dict(color='blue', width=2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot average agent wealth\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=avg_agent_wealth,\n",
    "            mode='lines',\n",
    "            name='Average Agent Wealth',\n",
    "            line=dict(color='green', width=2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title + str(len(recent_markets))+ ' Simulations',\n",
    "        xaxis_title=\"Time Steps\",\n",
    "        yaxis_title=\"Wealth\",\n",
    "        hovermode=\"x unified\",  # Improves interactivity\n",
    "        showlegend=True,\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    # Show the interactive plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "start_idx = max(0, 2900)\n",
    "plot_average_wealth(surplus_2,markets_2,agents_2, \"Second Average Market and Agent Wealth Across \", start_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Sharpe Ratios:   0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Sharpe Ratios: 100%|██████████| 3000/3000 [00:00<00:00, 3531.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sharpe Ratio for Agent across all simulations: 0.0519\n",
      "Average Sharpe Ratio for Market across all simulations: 0.0782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists to store Sharpe ratios for each episode for both the agent and the market\n",
    "agent_sharpe_ratios_2 = []\n",
    "market_sharpe_ratios_2 = []\n",
    "\n",
    "# Loop over each episode using the already populated `markets` and `agents` lists\n",
    "for episode_index in tqdm(range(len(markets_2)), desc=\"Calculating Sharpe Ratios\"):\n",
    "    # Get the agent and market values for the current episode\n",
    "    agent_values = agents_2[episode_index]\n",
    "    market_values = markets_2[episode_index]\n",
    "    \n",
    "    # Calculate returns for agent portfolio and market for each timestep in this episode\n",
    "    agent_returns = np.diff(agent_values) / agent_values[:-1]\n",
    "    market_returns = np.diff(market_values) / market_values[:-1]\n",
    "    \n",
    "    # Calculate the Sharpe ratio for the agent's portfolio in this episode\n",
    "    agent_sharpe_ratio = agent_returns.mean() / agent_returns.std() if agent_returns.std() != 0 else 0\n",
    "    agent_sharpe_ratios.append(agent_sharpe_ratio)\n",
    "    \n",
    "    # Calculate the Sharpe ratio for the market in this episode\n",
    "    market_sharpe_ratio = market_returns.mean() / market_returns.std() if market_returns.std() != 0 else 0\n",
    "    market_sharpe_ratios.append(market_sharpe_ratio)\n",
    "\n",
    "# Calculate the average Sharpe ratios across all episodes for both agent and market\n",
    "average_agent_sharpe_ratio = np.mean(agent_sharpe_ratios)\n",
    "average_market_sharpe_ratio = np.mean(market_sharpe_ratios)\n",
    "\n",
    "# Print the comparison\n",
    "print(f\"Average Sharpe Ratio for Agent across all simulations: {average_agent_sharpe_ratio:.4f}\")\n",
    "print(f\"Average Sharpe Ratio for Market across all simulations: {average_market_sharpe_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_scripts import generate_data as gd\n",
    "from data_scripts import plotting as pl\n",
    "import datetime\n",
    "\n",
    "tickers = ['MSFT', 'TGT', 'QCOM', 'MU', 'CAT']\n",
    "\n",
    "# Load simulation data for the selected stocks\n",
    "stock_data_dict = gd.load_simulation_data(tickers)\n",
    "\n",
    "# Get the combined training simulation data for the selected stocks\n",
    "combined_train_data = gd.get_combined_simulation(stock_data_dict, simulation_index=0, set_type='train')\n",
    "\n",
    "\n",
    "# Assuming combined_train_data is a NumPy array or a DataFrame\n",
    "num_days_train, num_ep_train = data = stock_data_dict[tickers[0]]['train'].shape\n",
    "num_days_test, num_ep_test = data = stock_data_dict[tickers[0]]['test'].shape\n",
    "\n",
    "def generate_time():\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return f\"{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n",
      "... loading checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Progress: 100%|██████████| 100/100 [35:29<00:00, 21.29s/it]\n"
     ]
    }
   ],
   "source": [
    "#raise Exception(\"Skip this cell\")\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from trading_envs.trading_env import TradingEnv\n",
    "from models.ddpg_agent import Agent\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Test on new test data\n",
    "reward_function = 'diff_sharpe_reward'\n",
    "test_simulations = num_ep_test\n",
    "test_days = num_days_test\n",
    "test_simulation_data = gd.get_combined_simulation(stock_data_dict, simulation_index=0, set_type='test')\n",
    "env_test = TradingEnv(stock_data=test_simulation_data, reward_function=reward_function)\n",
    "\n",
    "\n",
    "agent = Agent(alpha=0.0001, beta=0.001, input_dims=[env_test.observation_space.shape[0]], \n",
    "                tau=0.001, env=env_test, batch_size=64, layer1_size=400, layer2_size=300, \n",
    "                n_actions=env_test.action_space.shape[0])\n",
    "\n",
    "# Load the trained model # 9904_states_observed_20241029_232625_diff_sharpe_reward_value\n",
    "\n",
    "agent.load_models(suffix=f'9904_20241029_232625_diff_sharpe_reward')  # Load the most recent saved model\n",
    "\n",
    "actions_columns = ['Episode', 'TimeStep'] + [f'Action_{i}' for i in range(env_test.action_space.shape[0])]\n",
    "\n",
    "# DataFrames to store test results\n",
    "test_wealth_df = pd.DataFrame(columns=['Episode', 'TimeStep', 'Simulation', 'Wealth'])\n",
    "test_actions_df = pd.DataFrame(columns=actions_columns)\n",
    "test_rewards_df = pd.DataFrame(columns=['Episode', 'TimeStep', 'Reward'])\n",
    "states_columns = ['Episode', 'TimeStep'] + [f'State_{i}' for i in range(env_test.observation_space.shape[0])]\n",
    "test_states_df = pd.DataFrame(columns=states_columns)\n",
    "\n",
    "# Training loop only one time\n",
    "for episode in tqdm(range(test_simulations), desc=\"Testing Progress\"):\n",
    "    episode_wealth = []\n",
    "    episode_actions = []\n",
    "    episode_states = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Reset environment and variables\n",
    "    test_simulation_data = gd.get_combined_simulation(stock_data_dict, simulation_index=episode, set_type='test')\n",
    "    # Set the new environment with this training data\n",
    "    env = TradingEnv(stock_data=test_simulation_data, reward_function=reward_function)\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    time_step = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, _, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, state_, done)\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "\n",
    "        # Collect data\n",
    "        episode_wealth.append({\n",
    "            'Episode': episode, \n",
    "            'TimeStep': time_step, \n",
    "            'Simulation': episode,\n",
    "            'Wealth': env.get_portfolio_value()\n",
    "        })\n",
    "\n",
    "        state_record = {'Episode': episode, 'TimeStep': time_step}\n",
    "        state_record.update({f'State_{i}': s for i, s in enumerate(state)})\n",
    "        episode_states.append(state_record)\n",
    "\n",
    "        state = state_\n",
    "        time_step += 1\n",
    "\n",
    "    # Append episode data to DataFrames\n",
    "    test_wealth_df = pd.concat([test_wealth_df, pd.DataFrame(episode_wealth)], ignore_index=True)\n",
    "    test_states_df = pd.concat([test_states_df, pd.DataFrame(episode_states)], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "time = generate_time()\n",
    "time_first = time\n",
    "\n",
    "# Save with unique filename\n",
    "test_wealth_df.to_csv(f'data_save/wealth_over_time_test_{time}_{reward_function}.csv', index=False)\n",
    "test_states_df.to_csv(f'data_save/states_observed_test_{time}_{reward_function}.csv', index=False)\n",
    "\n",
    "\n",
    "save_num += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulations Progress: 100%|██████████| 99/99 [00:08<00:00, 12.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store surplus, market, and agent values for each simulation\n",
    "surplus = []\n",
    "markets = []\n",
    "agents = []\n",
    "\n",
    "# Load CSV data once\n",
    "agent_wealth = pd.read_csv('data_save/wealth_over_time_test_20241030_134110_diff_sharpe_reward.csv')\n",
    "states = pd.read_csv('data_save/states_observed_test_20241030_134110_diff_sharpe_reward.csv')\n",
    "\n",
    "#agent_wealth = pd.read_csv(f'data_save/wealth_over_time_test_{time}_{reward_function}.csv')\n",
    "#states = pd.read_csv(f'data_save/states_observed_test_{time}_{reward_function}.csv')\n",
    "\n",
    "# Loop over each simulation index\n",
    "for i in tqdm(range(agent_wealth['Episode'].max()), desc=\"Simulations Progress\"):\n",
    "    # Filter agent wealth data for the current episode\n",
    "    agent_wealth_e = agent_wealth[agent_wealth['Episode'] == i].set_index('TimeStep')\n",
    "    prices = states[states['Episode'] == i].set_index('TimeStep').filter(like='State_').values[:, :6]\n",
    "\n",
    "    # Preallocate arrays for each simulation's surplus\n",
    "    portfolio_surplus = np.empty(len(prices))\n",
    "    market = np.empty(len(prices))\n",
    "    agent = np.empty(len(prices))\n",
    "\n",
    "    # Iterate over the time steps\n",
    "    for n in range(len(prices)):\n",
    "        # Retrieve agent wealth data for the current timestep\n",
    "        agent_wealth_day = agent_wealth_e.loc[n, 'Wealth']\n",
    "        \n",
    "        # Ensure it's a scalar value\n",
    "        if isinstance(agent_wealth_day, pd.Series):\n",
    "            agent_wealth_day = agent_wealth_day.iloc[0]\n",
    "\n",
    "        # Calculate the current market value\n",
    "        market_price_day = prices[n]\n",
    "\n",
    "        # Distribute 1000 equally among the 5 stocks and cash with prices from the first day\n",
    "        dist_vec = 1000 / len(market_price_day) / prices[0]\n",
    "        market_wealth_day = np.sum(market_price_day * dist_vec)\n",
    "\n",
    "        # Ensure market_wealth_day is a scalar value\n",
    "        if isinstance(market_wealth_day, np.ndarray):\n",
    "            market_wealth_day = market_wealth_day.item()\n",
    "\n",
    "        # Calculate wealth difference (agent wealth - calculated wealth)\n",
    "        wealth_diff = agent_wealth_day - market_wealth_day\n",
    "\n",
    "        # Store the wealth difference in the preallocated arrays\n",
    "        portfolio_surplus[n] = np.round(wealth_diff, 2)\n",
    "        market[n] = np.round(market_wealth_day, 2)\n",
    "        agent[n] = np.round(agent_wealth_day, 2)\n",
    "\n",
    "    # Append the results for this simulation to the overall lists\n",
    "    surplus.append(portfolio_surplus)\n",
    "    markets.append(market)\n",
    "    agents.append(agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "# Function to plot the average agent wealth and market wealth across all simulations\n",
    "def plot_average_wealth(surplus, markets, agents, title, start_index=0):\n",
    "    # Calculate the average market and agent wealth across all simulations\n",
    "    recent_markets = markets[start_index:]\n",
    "    recent_agents = agents[start_index:]\n",
    "\n",
    "    avg_market_wealth = np.mean(recent_markets, axis=0)\n",
    "    avg_agent_wealth = np.mean(recent_agents, axis=0)\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot average market wealth\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=avg_market_wealth,\n",
    "            mode='lines',\n",
    "            name='Average Market Wealth',\n",
    "            line=dict(color='blue', width=2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot average agent wealth\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=avg_agent_wealth,\n",
    "            mode='lines',\n",
    "            name='Average Agent Wealth',\n",
    "            line=dict(color='green', width=2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title + str(len(recent_markets))+ ' Simulations',\n",
    "        xaxis_title=\"Time Steps\",\n",
    "        yaxis_title=\"Wealth\",\n",
    "        hovermode=\"x unified\",  # Improves interactivity\n",
    "        showlegend=True,\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    # Show the interactive plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "start_idx = max(0, 0)\n",
    "plot_average_wealth(surplus,markets,agents, \"Average Market Second test and Agent Wealth Across \", start_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Sharpe Ratios: 100%|██████████| 99/99 [00:00<00:00, 9903.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 Average Sharpe Ratio for Agent across all simulations: 0.0388\n",
      "Testing 1 Average Sharpe Ratio for Market across all simulations: 0.0778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists to store Sharpe ratios for each episode for both the agent and the market\n",
    "agent_sharpe_ratios = []\n",
    "market_sharpe_ratios = []\n",
    "\n",
    "# Loop over each episode using the already populated `markets` and `agents` lists\n",
    "for episode_index in tqdm(range(len(markets)), desc=\"Calculating Sharpe Ratios\"):\n",
    "    # Get the agent and market values for the current episode\n",
    "    agent_values = agents[episode_index]\n",
    "    market_values = markets[episode_index]\n",
    "    \n",
    "    # Calculate returns for agent portfolio and market for each timestep in this episode\n",
    "    agent_returns = np.diff(agent_values) / agent_values[:-1]\n",
    "    market_returns = np.diff(market_values) / market_values[:-1]\n",
    "    \n",
    "    # Calculate the Sharpe ratio for the agent's portfolio in this episode\n",
    "    agent_sharpe_ratio = agent_returns.mean() / agent_returns.std() if agent_returns.std() != 0 else 0\n",
    "    agent_sharpe_ratios.append(agent_sharpe_ratio)\n",
    "    \n",
    "    # Calculate the Sharpe ratio for the market in this episode\n",
    "    market_sharpe_ratio = market_returns.mean() / market_returns.std() if market_returns.std() != 0 else 0\n",
    "    market_sharpe_ratios.append(market_sharpe_ratio)\n",
    "\n",
    "# Calculate the average Sharpe ratios across all episodes for both agent and market\n",
    "average_agent_sharpe_ratio = np.mean(agent_sharpe_ratios)\n",
    "average_market_sharpe_ratio = np.mean(market_sharpe_ratios)\n",
    "\n",
    "# Print the comparison\n",
    "print(f\"Testing 1 Average Sharpe Ratio for Agent across all simulations: {average_agent_sharpe_ratio:.4f}\")\n",
    "print(f\"Testing 1 Average Sharpe Ratio for Market across all simulations: {average_market_sharpe_ratio:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
